{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqWXqEHtNqdtBFCoCiGgxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobsi1/Interviews/blob/main/%D0%94%D0%BB%D1%8F_%D1%81%D0%BE%D0%B1%D0%B5%D1%81%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnZHgA5Frj8o",
        "outputId": "fa21d809-a9fb-456d-bde3-d34956d5406a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.7)\n",
            "Collecting langchain-mistralai\n",
            "  Downloading langchain_mistralai-1.1.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting mistralai\n",
            "  Downloading mistralai-1.11.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting faiss-cpu==1.13.2\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu==1.13.2) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu==1.13.2) (25.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (0.4.3)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (0.28.1)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (0.22.2)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
            "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Collecting opentelemetry-semantic-conventions<0.60,>=0.59b0 (from mistralai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (6.0.3)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.25.2->langchain-mistralai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.6.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.37.0)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.32.4)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (5.29.5)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->mistralai) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.5.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_mistralai-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading mistralai-1.11.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.3.1-py3-none-any.whl (6.1 kB)\n",
            "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opentelemetry-proto, invoke, faiss-cpu, eval-type-backport, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, langchain-mistralai, mistralai\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-http\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-http 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-http-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed eval-type-backport-0.3.1 faiss-cpu-1.13.2 invoke-2.2.1 langchain-mistralai-1.1.1 mistralai-1.11.1 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-http-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-mistralai mistralai faiss-cpu==1.13.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai import Mistral, UserMessage, SystemMessage\n",
        "import json\n",
        "import re\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MISTRAL_API_KEY = userdata.get(\"MISTRAL_API_KEY\")\n",
        "client = Mistral(api_key=MISTRAL_API_KEY)\n",
        "\n",
        "def mistral_chat(system, user, history, mentors_thoughts = None):\n",
        "  if mentors_thoughts is None:\n",
        "    mentors_thoughts = []\n",
        "  messages_h = [{\"role\" : \"system\", \"content\" : system}] + history +  mentors_thoughts + [{\"role\" : \"user\",  \"content\" : user}]\n",
        "  resp = client.chat.complete(\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature = 0.4,\n",
        "    messages = messages_h\n",
        "\n",
        "  )\n",
        "  return resp.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "wiHDnhLqt4Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7341ac1-6913-4af8-86de-789654c603ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_input = input(\"\"\"Привет! Поделись, пожалуйста, информацией о себе в таком формате:\n",
        "Имя:\n",
        "Позиция:\n",
        "Грейд:\n",
        "Опыт:\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "context = {\n",
        "    \"name\" : context_input.split('Имя: ')[1].split('Позиция: ')[0].strip(),\n",
        "    \"pos\" : context_input.split('Позиция: ')[1].split('Грейд: ')[0].strip(),\n",
        "    \"grade\" : context_input.split('Грейд: ')[1].split('Опыт: ')[0].strip(),\n",
        "    \"exp\" : context_input.split('Опыт: ')[1].strip(),\n",
        "    \"history\" : []\n",
        "}\n",
        "\n",
        "state = {\n",
        "    \"participant_name\" : \"Рубцова Вера Вадимовна\",\n",
        "    \"turns\" : [],\n",
        "    #\"turn_id\" : 0,\n",
        "    \"final_feedback\": None\n",
        "}\n",
        "\n",
        "def add_state(user_message, internal_thoughts, agent_visible_message):\n",
        "  turn_id = len(state[\"turns\"]) + 1\n",
        "  state[\"turns\"].append({\n",
        "      \"turn_id\" : turn_id,\n",
        "      \"user_message\" : user_message,\n",
        "      \"internal_thoughts\": internal_thoughts,\n",
        "      \"agent_visible_message\" : agent_visible_message})\n",
        "\n",
        "def get_history(n = 20):\n",
        "  return context['history'][-n:]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Kde14VUcgE",
        "outputId": "844e1890-cdd6-47e1-d146-3f737ae2b0e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Привет! Поделись, пожалуйста, информацией о себе в таком формате: \n",
            "Имя: \n",
            "Позиция: \n",
            "Грейд: \n",
            "Опыт:\n",
            "Имя: Алекс Позиция: Backend Developer Грейд: Junior  Опыт: Пет-проекты на Django, немного SQL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interviwer_system = f\"\"\"Ты проводишь техническое собьеседование для кандидата на вакансию.\n",
        "  Веди себя профессионально и дружелюбно.\n",
        "  Задавай один вопрос за раз.\n",
        "  Если кандидат начинает задавать встречные вопросы уходя от темы, ответь сухо и кратко и сразу вернись к техническим вопросам.  Учитывай грейд опыт и направление кандидата, обращайся к нему по имени: Имя: {context[\"name\"]}, Грейд: {context[\"grade\"]}, Позиция: {context[\"pos\"]}, Опыт: {context[\"exp\"]}\n",
        "  Не анализируй и не оценивай ответы кандидата, это задача ментора, твоя задача - задавать вопросы, учитывая рекомендации ментора\"\"\"\n",
        "interviwer_system_que = f\"\"\"Ты проводишь техническое собьеседование для кандидата на вакансию.\n",
        "  Верни одну короткую мысль. Если ты получил от ментора несколько вариантов вопросов кандидату - обьясни свой выбор. Если получил один - вырази свою точку зрения на анализ ментора\"\"\"\n",
        "\n",
        "mentor_final_result = f\"\"\"Ты выступаешь ментором во время собеседования. Сформируй отчет на основе ответов и поведения кандидата по образцу:\n",
        "А. Вердикт (Decision)\n",
        "  ●\tGrade: Уровень кандидата (Junior / Middle / Senior) на основе ответов.\n",
        "  ●\tHiring Recommendation: (Hire / No Hire / Strong Hire).\n",
        "  ●\tConfidence Score: Насколько уверен в оценке (0-100%).\n",
        "Б. Анализ Hard Skills (Technical Review)\n",
        "Таблица или список тем, затронутых в интервью. - просто краткое название\n",
        "  ●\tConfirmed Skills: Темы, где кандидат дал точные ответы. Просто перечислить темы\n",
        "  ●\tKnowledge Gaps: Темы, где были допущены ошибки или кандидат сказал «не знаю». Тема - правильный ответ.\n",
        "○\tВажно: Здесь нужно привести правильный ответ только на те вопросы, которые пользователь завалил.\n",
        "В. Анализ Soft Skills & Communication\n",
        "  ●\tClarity: Насколько понятно излагает мысли.\n",
        "  ●\tHonesty: Пытался ли кандидат выкрутиться/соврать или честно признал незнание.\n",
        "  ●\tEngagement: Задавал ли встречные вопросы (если это было в сценарии).\n",
        "Г. Персональный Roadmap (Next Steps)\n",
        "  ●\tСписок конкретных тем/технологий, которые нужно подтянуть (на основе выявленных пробелов).\n",
        "Ответ нужен просто текстом\"\"\"\n",
        "\n",
        "mentor_system = f\"\"\"Ты выступаешь ментором во время собеседования.\n",
        "  Ты должен оценивать ответы и поведение кандидата и формировать записи для интервьюра\n",
        "  По шаблону:\n",
        "  \"краткое название темы вопроса интервьюера (под темой подразумевай раздел знаний)\",\n",
        "  \"result\" : \"confirmed/gap\",\n",
        "  \"right_answer\" : \"если result == gap, то приведи краткий верный ответ, иначе пусто\",\n",
        "  \"opinion\" : \"расскажи кратко, что было хорошо и плохо. предложи следующий вопрос и обьясни почему его предложил\"\n",
        "  ВСЕГДА указывай следующий вопрос В КОНЦЕ своего текста так: Следующий вопрос: ...\n",
        "  Учитывай грейд опыт и направление кандидата: Имя: {context[\"name\"]}, Грейд: {context[\"grade\"]}, Позиция: {context[\"pos\"]}, Опыт: {context[\"exp\"]}\"\"\""
      ],
      "metadata": {
        "id": "S8EwqOV8LLHp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.no_response_error import NoResponseError\n",
        "def interviwer(system, user, mentors_thoughts = None):\n",
        "  int_ans = mistral_chat(system, user, get_history(n = 20), mentors_thoughts = mentors_thoughts)\n",
        "  return int_ans\n",
        "\n",
        "def mentor(system, user):\n",
        "  men_ans = mistral_chat(system, user, get_history(n = 20))\n",
        "  return men_ans\n",
        "\n",
        "def main_function(user_message, last_que):\n",
        "  que_ans = f\"Вопрос: {last_que}\\n Ответ {user_message}\"\n",
        "  internal_thoughts = mentor(mentor_system, que_ans)\n",
        "\n",
        "  mentors_thoughts = [{\"role\" : \"system\",\n",
        "                      \"content\" : \"внутренние мысли ментора НЕЛЬЗЯ повторять или показывать кандидату\" + internal_thoughts}]\n",
        "\n",
        "\n",
        "\n",
        "  agent_visible_message = interviwer(interviwer_system, user_message, mentors_thoughts = mentors_thoughts)\n",
        "  interviwer_que = interviwer(interviwer_system_que, f\"Ты задал вопрос {agent_visible_message} Поясни на основе mentors_thoughts\", mentors_thoughts = mentors_thoughts)\n",
        "  internal_thoughts = f\"[Observer]: {internal_thoughts} \\n [Interviewer]: {interviwer_que}\"\n",
        "  add_state(user_message, internal_thoughts, last_que)\n",
        "\n",
        "  context['history'].append({\"role\" : \"user\",\n",
        "                            \"content\" : user_message})\n",
        "  context['history'].append({\"role\" : \"assistant\",\n",
        "                            \"content\" : agent_visible_message})\n",
        "\n",
        "  return agent_visible_message\n",
        "\n",
        "def start_interview(context_input):\n",
        "  mentor_start = mentor(mentor_system + \"Это первая информация по кандидату. Кратко подскажи интервьюеру с чего начать\", context_input)\n",
        "  mentor_start_thoughts = [{\"role\" : \"system\",\n",
        "                          \"content\" : \"внутренние мысли ментора НЕЛЬЗЯ повторять или показывать кандидату\" + mentor_start}]\n",
        "  int_ans = interviwer(interviwer_system, \"Начни и задай первый вопрос\", mentors_thoughts = mentor_start_thoughts)\n",
        "\n",
        "  #add_state(context_input, mentor_start, int_ans)\n",
        "  context['history'].append({\"role\" : \"assistant\",\n",
        "                            \"content\" : int_ans})\n",
        "  return int_ans\n",
        "\n",
        "print(\"Для остановки напиши Стоп интервью\")\n",
        "first_que = start_interview(context_input)\n",
        "last_que = first_que\n",
        "print(first_que)\n",
        "\n",
        "while True:\n",
        "  user_message = input()\n",
        "  if user_message == 'Стоп интервью':\n",
        "    last = \"\"\n",
        "    add_state(user_message, \"Заканчиваем. Формируем отчет\", last)\n",
        "    fin = mentor(mentor_final_result, \"Сформируй отчет\")\n",
        "    state[\"final_feedback\"] = fin\n",
        "    print(fin)\n",
        "    break\n",
        "  ans = main_function(user_message, last_que)\n",
        "  last_que = ans\n",
        "  print(ans)\n",
        "with open('interview_log.json', 'w', encoding='utf-8') as s:\n",
        "  json.dump(state, s, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "dYlh3txz0Q8_",
        "outputId": "ac7d71bc-3089-4b8e-a2aa-0db1cca00a95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Для остановки напиши Стоп интервью\n",
            "Алекс, давай теперь поговорим про связи между моделями.\n",
            "\n",
            "Представь, что у модели `Order` есть связь с моделью `Product` через `ManyToManyField`. Как бы ты написал запрос, чтобы получить все заказы пользователя с `id=5` и сразу подгрузить связанные продукты, избежав проблемы N+1 запросов?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4088282016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muser_message\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Стоп интервью'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}