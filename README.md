# Interviews

**Общая идея и сценарий работы**
Код реализует консольное (интерактивное) техническое собеседование с двумя LLM-агентами на базе Mistral:
Interviewer (Интервьюер) — ведёт диалог с кандидатом: задаёт вопросы по одному, держит тон, не оценивает ответы.
Mentor (Ментор/Наблюдатель) — оценивает ответы кандидата, фиксирует “confirmed/gap”, даёт правильный ответ при “gap” и обязательно предлагает следующий вопрос.

**Сценарий:**
Пользователь вводит “карточку кандидата” (имя/позиция/грейд/опыт).
Ментор подсказывает, с чего начать.
Интервьюер задаёт первый вопрос.

**На каждом ходе:**
кандидат отвечает,
ментор анализирует,
интервьюер задаёт следующий вопрос, учитывая рекомендации ментора,
всё логируется.
**По команде Стоп интервью ментор формирует финальный отчёт и он сохраняется в JSON.**

Центральная функция — mistral_chat(): она собирает единый массив сообщений и делает вызов client.chat.complete(...) к модели mistral-large-latest с temperature=0.4.

**Память (memory): что хранится и как используется**
 
  **“Краткосрочная память диалога” — context['history']**

Создаётся структура context с полями кандидата и history (список сообщений). 
для_собеседований
history пополняется после каждого хода:
сначала добавляется ответ кандидата как {"role":"user","content":...},
затем сообщение интервьюера (обычно следующий вопрос) как {"role":"assistant","content":...}. 
для_собеседований
Чтобы ограничить размер контекста, используется “скользящее окно”:
get_history(n=20) возвращает последние 20 сообщений. 
для_собеседований
То есть память не суммаризируется, а просто “обрезается” по хвосту.

  **“Оперативная память для аудита/логов” — state**
Отдельно ведётся state:
participant_name (захардкожен),
turns — список ходов,
final_feedback — финальный отчёт ментора. 
для_собеседований
Функция add_state(...) добавляет запись хода: turn_id, user_message, internal_thoughts, agent_visible_message. 
для_собеседований
В конце state сериализуется в interview_log.json. 
для_собеседований
